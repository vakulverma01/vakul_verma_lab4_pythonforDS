{"cells":[{"cell_type":"markdown","id":"83f26a29","metadata":{"id":"83f26a29"},"source":["# Unsupervised Lab Session"]},{"cell_type":"markdown","id":"8ea571d1","metadata":{"id":"8ea571d1"},"source":["## Learning outcomes:\n","- Exploratory data analysis and data preparation for model building.\n","- PCA for dimensionality reduction.\n","- K-means and Agglomerative Clustering"]},{"cell_type":"markdown","id":"fd7f778a","metadata":{"id":"fd7f778a"},"source":["## Problem Statement\n","Based on the given marketing campigan dataset, segment the similar customers into suitable clusters. Analyze the clusters and provide your insights to help the organization promote their business."]},{"cell_type":"markdown","id":"33b58f8f","metadata":{"id":"33b58f8f"},"source":["## Context:\n","- Customer Personality Analysis is a detailed analysis of a company’s ideal customers. It helps a business to better understand its customers and makes it easier for them to modify products according to the specific needs, behaviors and concerns of different types of customers.\n","- Customer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company’s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment."]},{"cell_type":"markdown","id":"867166aa","metadata":{"id":"867166aa"},"source":["## About dataset\n","- Source: https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis?datasetId=1546318&sortBy=voteCount\n","\n","### Attribute Information:\n","- ID: Customer's unique identifier\n","- Year_Birth: Customer's birth year\n","- Education: Customer's education level\n","- Marital_Status: Customer's marital status\n","- Income: Customer's yearly household income\n","- Kidhome: Number of children in customer's household\n","- Teenhome: Number of teenagers in customer's household\n","- Dt_Customer: Date of customer's enrollment with the company\n","- Recency: Number of days since customer's last purchase\n","- Complain: 1 if the customer complained in the last 2 years, 0 otherwise\n","- MntWines: Amount spent on wine in last 2 years\n","- MntFruits: Amount spent on fruits in last 2 years\n","- MntMeatProducts: Amount spent on meat in last 2 years\n","- MntFishProducts: Amount spent on fish in last 2 years\n","- MntSweetProducts: Amount spent on sweets in last 2 years\n","- MntGoldProds: Amount spent on gold in last 2 years\n","- NumDealsPurchases: Number of purchases made with a discount\n","- AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n","- AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n","- AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n","- AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n","- AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n","- Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n","- NumWebPurchases: Number of purchases made through the company’s website\n","- NumCatalogPurchases: Number of purchases made using a catalogue\n","- NumStorePurchases: Number of purchases made directly in stores\n","- NumWebVisitsMonth: Number of visits to company’s website in the last month"]},{"cell_type":"markdown","id":"5a830406","metadata":{"id":"5a830406"},"source":["### 1. Import required libraries"]},{"cell_type":"code","execution_count":null,"id":"d65c5528","metadata":{"id":"d65c5528"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"markdown","id":"c80eb960","metadata":{"id":"c80eb960"},"source":["### 2. Load the CSV file (i.e marketing.csv) and display the first 5 rows of the dataframe. Check the shape and info of the dataset."]},{"cell_type":"code","execution_count":1,"id":"1caebc10","metadata":{"id":"1caebc10"},"outputs":[{"name":"stdout","output_type":"stream","text":["     ID  Year_Birth   Education Marital_Status   Income  Kidhome  Teenhome  \\\n","0  5524        1957  Graduation         Single  58138.0        0         0   \n","1  2174        1954  Graduation         Single  46344.0        1         1   \n","2  4141        1965  Graduation       Together  71613.0        0         0   \n","3  6182        1984  Graduation       Together  26646.0        1         0   \n","4  5324        1981         PhD        Married  58293.0        1         0   \n","\n","  Dt_Customer  Recency  MntWines  ...  NumCatalogPurchases  NumStorePurchases  \\\n","0    4/9/2012       58       635  ...                   10                  4   \n","1    8/3/2014       38        11  ...                    1                  2   \n","2  21-08-2013       26       426  ...                    2                 10   \n","3   10/2/2014       26        11  ...                    0                  4   \n","4  19-01-2014       94       173  ...                    3                  6   \n","\n","   NumWebVisitsMonth  AcceptedCmp3  AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  \\\n","0                  7             0             0             0             0   \n","1                  5             0             0             0             0   \n","2                  4             0             0             0             0   \n","3                  6             0             0             0             0   \n","4                  5             0             0             0             0   \n","\n","   AcceptedCmp2  Complain  Response  \n","0             0         0         1  \n","1             0         0         0  \n","2             0         0         0  \n","3             0         0         0  \n","4             0         0         0  \n","\n","[5 rows x 27 columns]\n","\n","Shape of the data (rows, columns):\n","(2240, 27)\n","\n","General information about the dataframe:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 2240 entries, 0 to 2239\n","Data columns (total 27 columns):\n"," #   Column               Non-Null Count  Dtype  \n","---  ------               --------------  -----  \n"," 0   ID                   2240 non-null   int64  \n"," 1   Year_Birth           2240 non-null   int64  \n"," 2   Education            2240 non-null   object \n"," 3   Marital_Status       2240 non-null   object \n"," 4   Income               2216 non-null   float64\n"," 5   Kidhome              2240 non-null   int64  \n"," 6   Teenhome             2240 non-null   int64  \n"," 7   Dt_Customer          2240 non-null   object \n"," 8   Recency              2240 non-null   int64  \n"," 9   MntWines             2240 non-null   int64  \n"," 10  MntFruits            2240 non-null   int64  \n"," 11  MntMeatProducts      2240 non-null   int64  \n"," 12  MntFishProducts      2240 non-null   int64  \n"," 13  MntSweetProducts     2240 non-null   int64  \n"," 14  MntGoldProds         2240 non-null   int64  \n"," 15  NumDealsPurchases    2240 non-null   int64  \n"," 16  NumWebPurchases      2240 non-null   int64  \n"," 17  NumCatalogPurchases  2240 non-null   int64  \n"," 18  NumStorePurchases    2240 non-null   int64  \n"," 19  NumWebVisitsMonth    2240 non-null   int64  \n"," 20  AcceptedCmp3         2240 non-null   int64  \n"," 21  AcceptedCmp4         2240 non-null   int64  \n"," 22  AcceptedCmp5         2240 non-null   int64  \n"," 23  AcceptedCmp1         2240 non-null   int64  \n"," 24  AcceptedCmp2         2240 non-null   int64  \n"," 25  Complain             2240 non-null   int64  \n"," 26  Response             2240 non-null   int64  \n","dtypes: float64(1), int64(23), object(3)\n","memory usage: 472.6+ KB\n","None\n"]}],"source":["import pandas as pd\n","file_path = 'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Display a sample of five rows\n","print(df.head()) \n","# Check the shape of the data\n","print(\"\\nShape of the data (rows, columns):\")\n","print(df.shape)\n","\n","# Check general information about the dataframe\n","print(\"\\nGeneral information about the dataframe:\")\n","print(df.info())"]},{"cell_type":"markdown","id":"9ef75724","metadata":{"id":"9ef75724"},"source":["### 3. Check the percentage of missing values? If there is presence of missing values, treat them accordingly."]},{"cell_type":"code","execution_count":2,"id":"f2c231df","metadata":{"id":"f2c231df"},"outputs":[{"name":"stdout","output_type":"stream","text":["Missing Value Information:\n","                     Missing Values  Percentage\n","ID                                0    0.000000\n","Year_Birth                        0    0.000000\n","Education                         0    0.000000\n","Marital_Status                    0    0.000000\n","Income                           24    1.071429\n","Kidhome                           0    0.000000\n","Teenhome                          0    0.000000\n","Dt_Customer                       0    0.000000\n","Recency                           0    0.000000\n","MntWines                          0    0.000000\n","MntFruits                         0    0.000000\n","MntMeatProducts                   0    0.000000\n","MntFishProducts                   0    0.000000\n","MntSweetProducts                  0    0.000000\n","MntGoldProds                      0    0.000000\n","NumDealsPurchases                 0    0.000000\n","NumWebPurchases                   0    0.000000\n","NumCatalogPurchases               0    0.000000\n","NumStorePurchases                 0    0.000000\n","NumWebVisitsMonth                 0    0.000000\n","AcceptedCmp3                      0    0.000000\n","AcceptedCmp4                      0    0.000000\n","AcceptedCmp5                      0    0.000000\n","AcceptedCmp1                      0    0.000000\n","AcceptedCmp2                      0    0.000000\n","Complain                          0    0.000000\n","Response                          0    0.000000\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_6420\\2582186289.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df[col].fillna(df[col].mean(), inplace=True)\n","C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_6420\\2582186289.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df[col].fillna(df[col].mode()[0], inplace=True)\n"]}],"source":["import pandas as pd\n","\n","# Correcting the file path\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Check for missing values\n","missing_values = df.isnull().sum()\n","\n","# Calculate percentage of missing values\n","missing_percentage = (missing_values / len(df)) * 100\n","\n","# Combine missing values and percentages into a DataFrame for clarity\n","missing_info = pd.DataFrame({\n","    'Missing Values': missing_values,\n","    'Percentage': missing_percentage\n","})\n","\n","print(\"Missing Value Information:\")\n","print(missing_info)\n","\n","# Handle missing values based on your chosen method (e.g., imputation or dropping rows)\n","# Example: Impute missing values in numeric columns with mean\n","numeric_columns = df.select_dtypes(include='number').columns\n","for col in numeric_columns:\n","    df[col].fillna(df[col].mean(), inplace=True)\n","\n","# Example: Impute missing values in categorical columns with mode\n","categorical_columns = df.select_dtypes(include='object').columns\n","for col in categorical_columns:\n","    df[col].fillna(df[col].mode()[0], inplace=True)\n","\n","# Alternatively, you can drop rows with any missing values\n","# df.dropna(axis=0, inplace=True)\n","\n","# Now your dataset (`df`) should be processed with missing values handled appropriately\n"]},{"cell_type":"markdown","id":"86f3709e","metadata":{"id":"86f3709e"},"source":["### 4. Check if there are any duplicate records in the dataset? If any drop them."]},{"cell_type":"code","execution_count":3,"id":"2970671a","metadata":{"id":"2970671a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of duplicate records: 0\n","Dataset shape after removing duplicates: (2240, 27)\n"]}],"source":["import pandas as pd\n","\n","# Correcting the file path\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Check for duplicate records\n","duplicate_rows = df[df.duplicated()]\n","\n","# Print the number of duplicate records\n","print(f\"Number of duplicate records: {duplicate_rows.shape[0]}\")\n","\n","# Optionally, display the duplicate rows\n","if duplicate_rows.shape[0] > 0:\n","    print(\"\\nDuplicate Rows:\")\n","    print(duplicate_rows)\n","\n","# Drop duplicate records\n","df.drop_duplicates(inplace=True)\n","\n","# Confirm duplicates have been dropped\n","print(f\"Dataset shape after removing duplicates: {df.shape}\")\n","\n","# Now your dataset (`df`) should be processed with duplicate records removed\n"]},{"cell_type":"markdown","id":"3a6f2b5a","metadata":{"id":"3a6f2b5a"},"source":["### 5. Drop the columns which you think redundant for the analysis "]},{"cell_type":"code","execution_count":6,"id":"a9ca818b","metadata":{"id":"a9ca818b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Columns before dropping redundant ones:\n","Index(['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n","       'Teenhome', 'Dt_Customer', 'Recency', 'MntWines', 'MntFruits',\n","       'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n","       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n","       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n","       'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1',\n","       'AcceptedCmp2', 'Complain', 'Response'],\n","      dtype='object')\n","\n","Columns after dropping redundant ones:\n","Index(['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n","       'Teenhome', 'Dt_Customer', 'Recency', 'MntWines', 'MntFruits',\n","       'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n","       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n","       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n","       'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1',\n","       'AcceptedCmp2', 'Complain', 'Response'],\n","      dtype='object')\n"]}],"source":["import pandas as pd\n","\n","# Correcting the file path\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Display current columns\n","print(\"Columns before dropping redundant ones:\")\n","print(df.columns)\n","\n","# Example: Dropping columns with constant values\n","constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n","df.drop(columns=constant_columns, inplace=True)\n","\n","# Example: Dropping columns with high missing values (threshold set to 30%)\n","missing_threshold = 30  # Set your own threshold as needed\n","missing_percentage = (df.isnull().sum() / len(df)) * 100\n","high_missing_columns = missing_percentage[missing_percentage > missing_threshold].index.tolist()\n","df.drop(columns=high_missing_columns, inplace=True)\n","\n","# Example: Dropping columns that are not relevant to analysis\n","columns_to_drop = ['IrrelevantColumn1', 'IrrelevantColumn2']\n","\n","# Check if columns_to_drop exist in df.columns before dropping\n","columns_to_drop_existing = [col for col in columns_to_drop if col in df.columns]\n","df.drop(columns=columns_to_drop_existing, inplace=True)\n","\n","# Display columns after dropping redundant ones\n","print(\"\\nColumns after dropping redundant ones:\")\n","print(df.columns)\n"]},{"cell_type":"markdown","id":"4ff0a112","metadata":{"id":"4ff0a112"},"source":["### 6. Check the unique categories in the column 'Marital_Status'\n","- i) Group categories 'Married', 'Together' as 'relationship'\n","- ii) Group categories 'Divorced', 'Widow', 'Alone', 'YOLO', and 'Absurd' as 'Single'."]},{"cell_type":"code","execution_count":7,"id":"eb1be519","metadata":{"id":"eb1be519"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique categories in 'Marital_Status':\n","['Single' 'Together' 'Married' 'Divorced' 'Widow' 'Alone' 'Absurd' 'YOLO']\n","\n","Updated unique categories in 'Marital_Status':\n","['Single' 'relationship']\n"]}],"source":["import pandas as pd\n","\n","# Correcting the file path\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Check unique categories in 'Marital_Status'\n","unique_categories = df['Marital_Status'].unique()\n","print(\"Unique categories in 'Marital_Status':\")\n","print(unique_categories)\n","\n","# Group categories as specified\n","category_mapping = {\n","    'Married': 'relationship',\n","    'Together': 'relationship',\n","    'Divorced': 'Single',\n","    'Widow': 'Single',\n","    'Alone': 'Single',\n","    'YOLO': 'Single',\n","    'Absurd': 'Single'\n","}\n","\n","# Replace categories in 'Marital_Status' column\n","df['Marital_Status'] = df['Marital_Status'].replace(category_mapping)\n","\n","# Verify the changes\n","updated_unique_categories = df['Marital_Status'].unique()\n","print(\"\\nUpdated unique categories in 'Marital_Status':\")\n","print(updated_unique_categories)\n"]},{"cell_type":"markdown","id":"9566bfbe","metadata":{"id":"9566bfbe"},"source":["### 7. Group the columns 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', and 'MntGoldProds' as 'Total_Expenses'"]},{"cell_type":"code","execution_count":8,"id":"3c3fa800","metadata":{"id":"3c3fa800"},"outputs":[{"name":"stdout","output_type":"stream","text":["     ID  Year_Birth   Education Marital_Status   Income  Kidhome  Teenhome  \\\n","0  5524        1957  Graduation         Single  58138.0        0         0   \n","1  2174        1954  Graduation         Single  46344.0        1         1   \n","2  4141        1965  Graduation       Together  71613.0        0         0   \n","3  6182        1984  Graduation       Together  26646.0        1         0   \n","4  5324        1981         PhD        Married  58293.0        1         0   \n","\n","  Dt_Customer  Recency  MntWines  ...  NumStorePurchases  NumWebVisitsMonth  \\\n","0    4/9/2012       58       635  ...                  4                  7   \n","1    8/3/2014       38        11  ...                  2                  5   \n","2  21-08-2013       26       426  ...                 10                  4   \n","3   10/2/2014       26        11  ...                  4                  6   \n","4  19-01-2014       94       173  ...                  6                  5   \n","\n","   AcceptedCmp3  AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  AcceptedCmp2  \\\n","0             0             0             0             0             0   \n","1             0             0             0             0             0   \n","2             0             0             0             0             0   \n","3             0             0             0             0             0   \n","4             0             0             0             0             0   \n","\n","   Complain  Response  Total_Expenses  \n","0         0         1            1617  \n","1         0         0              27  \n","2         0         0             776  \n","3         0         0              53  \n","4         0         0             422  \n","\n","[5 rows x 28 columns]\n"]}],"source":["import pandas as pd\n","\n","# Correcting the file path\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Specify columns to be summed\n","expense_columns = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n","\n","# Create 'Total_Expenses' column by summing specified columns\n","df['Total_Expenses'] = df[expense_columns].sum(axis=1)\n","\n","# Display the updated DataFrame with 'Total_Expenses'\n","print(df.head())  # Displaying the first few rows to verify\n","\n","# Optionally, you can drop the individual expense columns if needed\n","# df.drop(columns=expense_columns, inplace=True)\n"]},{"cell_type":"markdown","id":"bf0cd083","metadata":{"id":"bf0cd083"},"source":["### 8. Group the columns 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', and 'NumDealsPurchases' as 'Num_Total_Purchases'"]},{"cell_type":"code","execution_count":9,"id":"9c535ede","metadata":{"id":"9c535ede"},"outputs":[{"name":"stdout","output_type":"stream","text":["     ID  Year_Birth   Education Marital_Status   Income  Kidhome  Teenhome  \\\n","0  5524        1957  Graduation         Single  58138.0        0         0   \n","1  2174        1954  Graduation         Single  46344.0        1         1   \n","2  4141        1965  Graduation       Together  71613.0        0         0   \n","3  6182        1984  Graduation       Together  26646.0        1         0   \n","4  5324        1981         PhD        Married  58293.0        1         0   \n","\n","  Dt_Customer  Recency  MntWines  ...  NumStorePurchases  NumWebVisitsMonth  \\\n","0    4/9/2012       58       635  ...                  4                  7   \n","1    8/3/2014       38        11  ...                  2                  5   \n","2  21-08-2013       26       426  ...                 10                  4   \n","3   10/2/2014       26        11  ...                  4                  6   \n","4  19-01-2014       94       173  ...                  6                  5   \n","\n","   AcceptedCmp3  AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  AcceptedCmp2  \\\n","0             0             0             0             0             0   \n","1             0             0             0             0             0   \n","2             0             0             0             0             0   \n","3             0             0             0             0             0   \n","4             0             0             0             0             0   \n","\n","   Complain  Response  Num_Total_Purchases  \n","0         0         1                   25  \n","1         0         0                    6  \n","2         0         0                   21  \n","3         0         0                    8  \n","4         0         0                   19  \n","\n","[5 rows x 28 columns]\n"]}],"source":["import pandas as pd\n","\n","# Correcting the file path\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Specify columns to be summed\n","purchase_columns = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases']\n","\n","# Create 'Num_Total_Purchases' column by summing specified columns\n","df['Num_Total_Purchases'] = df[purchase_columns].sum(axis=1)\n","\n","# Display the updated DataFrame with 'Num_Total_Purchases'\n","print(df.head())  # Displaying the first few rows to verify\n","\n","# Optionally, you can drop the individual purchase columns if needed\n","# df.drop(columns=purchase_columns, inplace=True)\n"]},{"cell_type":"markdown","id":"52d2dca5","metadata":{"id":"52d2dca5"},"source":["### 9. Group the columns 'Kidhome' and 'Teenhome' as 'Kids'"]},{"cell_type":"code","execution_count":10,"id":"f7c861a1","metadata":{"id":"f7c861a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["     ID  Year_Birth   Education Marital_Status   Income  Kidhome  Teenhome  \\\n","0  5524        1957  Graduation         Single  58138.0        0         0   \n","1  2174        1954  Graduation         Single  46344.0        1         1   \n","2  4141        1965  Graduation       Together  71613.0        0         0   \n","3  6182        1984  Graduation       Together  26646.0        1         0   \n","4  5324        1981         PhD        Married  58293.0        1         0   \n","\n","  Dt_Customer  Recency  MntWines  ...  NumStorePurchases  NumWebVisitsMonth  \\\n","0    4/9/2012       58       635  ...                  4                  7   \n","1    8/3/2014       38        11  ...                  2                  5   \n","2  21-08-2013       26       426  ...                 10                  4   \n","3   10/2/2014       26        11  ...                  4                  6   \n","4  19-01-2014       94       173  ...                  6                  5   \n","\n","   AcceptedCmp3  AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  AcceptedCmp2  \\\n","0             0             0             0             0             0   \n","1             0             0             0             0             0   \n","2             0             0             0             0             0   \n","3             0             0             0             0             0   \n","4             0             0             0             0             0   \n","\n","   Complain  Response  Kids  \n","0         0         1     0  \n","1         0         0     2  \n","2         0         0     0  \n","3         0         0     1  \n","4         0         0     1  \n","\n","[5 rows x 28 columns]\n"]}],"source":["import pandas as pd\n","\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Create 'Kids' column by summing 'Kidhome' and 'Teenhome'\n","df['Kids'] = df['Kidhome'] + df['Teenhome']\n","\n","# Display the updated DataFrame with 'Kids'\n","print(df.head())  # Displaying the first few rows to verify\n","\n","# Optionally, you can drop the individual 'Kidhome' and 'Teenhome' columns if needed\n","# df.drop(columns=['Kidhome', 'Teenhome'], inplace=True)\n"]},{"cell_type":"markdown","id":"36f67474","metadata":{"id":"36f67474"},"source":["### 10. Group columns 'AcceptedCmp1 , 2 , 3 , 4, 5' and 'Response' as 'TotalAcceptedCmp'"]},{"cell_type":"code","execution_count":11,"id":"ecc9109f","metadata":{"id":"ecc9109f"},"outputs":[{"name":"stdout","output_type":"stream","text":["     ID  Year_Birth   Education Marital_Status   Income  Kidhome  Teenhome  \\\n","0  5524        1957  Graduation         Single  58138.0        0         0   \n","1  2174        1954  Graduation         Single  46344.0        1         1   \n","2  4141        1965  Graduation       Together  71613.0        0         0   \n","3  6182        1984  Graduation       Together  26646.0        1         0   \n","4  5324        1981         PhD        Married  58293.0        1         0   \n","\n","  Dt_Customer  Recency  MntWines  ...  NumStorePurchases  NumWebVisitsMonth  \\\n","0    4/9/2012       58       635  ...                  4                  7   \n","1    8/3/2014       38        11  ...                  2                  5   \n","2  21-08-2013       26       426  ...                 10                  4   \n","3   10/2/2014       26        11  ...                  4                  6   \n","4  19-01-2014       94       173  ...                  6                  5   \n","\n","   AcceptedCmp3  AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  AcceptedCmp2  \\\n","0             0             0             0             0             0   \n","1             0             0             0             0             0   \n","2             0             0             0             0             0   \n","3             0             0             0             0             0   \n","4             0             0             0             0             0   \n","\n","   Complain  Response  TotalAcceptedCmp  \n","0         0         1                 1  \n","1         0         0                 0  \n","2         0         0                 0  \n","3         0         0                 0  \n","4         0         0                 0  \n","\n","[5 rows x 28 columns]\n"]}],"source":["import pandas as pd\n","\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Specify columns to be summed\n","accepted_cmp_columns = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']\n","\n","# Create 'TotalAcceptedCmp' column by summing specified columns\n","df['TotalAcceptedCmp'] = df[accepted_cmp_columns].sum(axis=1)\n","\n","# Display the updated DataFrame with 'TotalAcceptedCmp'\n","print(df.head())  # Displaying the first few rows to verify\n","\n","# Optionally, you can drop the individual accepted campaign columns if needed\n","# df.drop(columns=accepted_cmp_columns, inplace=True)\n"]},{"cell_type":"markdown","id":"886bfb08","metadata":{"id":"886bfb08"},"source":["### 11. Drop those columns which we have used above for obtaining new features"]},{"cell_type":"code","execution_count":12,"id":"e853e663","metadata":{"id":"e853e663"},"outputs":[{"name":"stdout","output_type":"stream","text":["     ID  Year_Birth   Education Marital_Status   Income Dt_Customer  Recency  \\\n","0  5524        1957  Graduation         Single  58138.0    4/9/2012       58   \n","1  2174        1954  Graduation         Single  46344.0    8/3/2014       38   \n","2  4141        1965  Graduation       Together  71613.0  21-08-2013       26   \n","3  6182        1984  Graduation       Together  26646.0   10/2/2014       26   \n","4  5324        1981         PhD        Married  58293.0  19-01-2014       94   \n","\n","   NumWebVisitsMonth  Complain  TotalAcceptedCmp  \n","0                  7         0                 1  \n","1                  5         0                 0  \n","2                  4         0                 0  \n","3                  6         0                 0  \n","4                  5         0                 0  \n"]}],"source":["# Assuming 'df' is your DataFrame and you have already created the new features\n","\n","# Drop original columns used for creating 'Total_Expenses'\n","expense_columns = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n","df.drop(columns=expense_columns, inplace=True)\n","\n","# Drop original columns used for creating 'Num_Total_Purchases'\n","purchase_columns = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases']\n","df.drop(columns=purchase_columns, inplace=True)\n","\n","# Drop original columns used for creating 'Kids'\n","df.drop(columns=['Kidhome', 'Teenhome'], inplace=True)\n","\n","# Drop original columns used for creating 'TotalAcceptedCmp'\n","accepted_cmp_columns = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']\n","df.drop(columns=accepted_cmp_columns, inplace=True)\n","\n","# Optionally, you can print the updated DataFrame to verify\n","print(df.head())\n"]},{"cell_type":"markdown","id":"4225ced7","metadata":{"id":"4225ced7"},"source":["### 12. Extract 'age' using the column 'Year_Birth' and then drop the column 'Year_birth'"]},{"cell_type":"code","execution_count":15,"id":"d517611e","metadata":{"id":"d517611e"},"outputs":[{"name":"stdout","output_type":"stream","text":["     ID  Year_Birth   Education Marital_Status   Income  Kidhome  Teenhome  \\\n","0  5524        1957  Graduation         Single  58138.0        0         0   \n","1  2174        1954  Graduation         Single  46344.0        1         1   \n","2  4141        1965  Graduation       Together  71613.0        0         0   \n","3  6182        1984  Graduation       Together  26646.0        1         0   \n","4  5324        1981         PhD        Married  58293.0        1         0   \n","\n","  Dt_Customer  Recency  MntWines  ...  NumCatalogPurchases  NumStorePurchases  \\\n","0    4/9/2012       58       635  ...                   10                  4   \n","1    8/3/2014       38        11  ...                    1                  2   \n","2  21-08-2013       26       426  ...                    2                 10   \n","3   10/2/2014       26        11  ...                    0                  4   \n","4  19-01-2014       94       173  ...                    3                  6   \n","\n","   NumWebVisitsMonth  AcceptedCmp3  AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  \\\n","0                  7             0             0             0             0   \n","1                  5             0             0             0             0   \n","2                  4             0             0             0             0   \n","3                  6             0             0             0             0   \n","4                  5             0             0             0             0   \n","\n","   AcceptedCmp2  Complain  Response  \n","0             0         0         1  \n","1             0         0         0  \n","2             0         0         0  \n","3             0         0         0  \n","4             0         0         0  \n","\n","[5 rows x 27 columns]\n","     ID   Education Marital_Status   Income  Kidhome  Teenhome Dt_Customer  \\\n","0  5524  Graduation         Single  58138.0        0         0    4/9/2012   \n","1  2174  Graduation         Single  46344.0        1         1    8/3/2014   \n","2  4141  Graduation       Together  71613.0        0         0  21-08-2013   \n","3  6182  Graduation       Together  26646.0        1         0   10/2/2014   \n","4  5324         PhD        Married  58293.0        1         0  19-01-2014   \n","\n","   Recency  MntWines  MntFruits  ...  NumStorePurchases  NumWebVisitsMonth  \\\n","0       58       635         88  ...                  4                  7   \n","1       38        11          1  ...                  2                  5   \n","2       26       426         49  ...                 10                  4   \n","3       26        11          4  ...                  4                  6   \n","4       94       173         43  ...                  6                  5   \n","\n","   AcceptedCmp3  AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  AcceptedCmp2  \\\n","0             0             0             0             0             0   \n","1             0             0             0             0             0   \n","2             0             0             0             0             0   \n","3             0             0             0             0             0   \n","4             0             0             0             0             0   \n","\n","   Complain  Response  age  \n","0         0         1   67  \n","1         0         0   70  \n","2         0         0   59  \n","3         0         0   40  \n","4         0         0   43  \n","\n","[5 rows x 27 columns]\n"]}],"source":["import pandas as pd\n","\n","# Correcting the file path\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Print the first few rows of the dataset to inspect its structure\n","print(df.head())\n","\n","# Check if 'Year_Birth' column exists in the dataset\n","if 'Year_Birth' in df.columns:\n","    # Calculate 'age' from 'Year_Birth' column\n","    current_year = 2024\n","    df['age'] = current_year - df['Year_Birth']\n","\n","    # Drop 'Year_Birth' column\n","    df.drop(columns=['Year_Birth'], inplace=True)\n","\n","    # Display the updated DataFrame with 'age' and without 'Year_Birth'\n","    print(df.head())\n","else:\n","    print(\"Column 'Year_Birth' not found in the dataset.\")\n"]},{"cell_type":"markdown","id":"f2d3c92d","metadata":{"id":"f2d3c92d"},"source":["### 13. Encode the categorical variables in the dataset"]},{"cell_type":"code","execution_count":16,"id":"030cfc32","metadata":{"id":"030cfc32"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Marital_Status  Marital_Status_LabelEncoded\n","0         Single                            4\n","1         Single                            4\n","2       Together                            5\n","3       Together                            5\n","4        Married                            3\n","     ID  Year_Birth Marital_Status   Income  Kidhome  Teenhome Dt_Customer  \\\n","0  5524        1957         Single  58138.0        0         0    4/9/2012   \n","1  2174        1954         Single  46344.0        1         1    8/3/2014   \n","2  4141        1965       Together  71613.0        0         0  21-08-2013   \n","3  6182        1984       Together  26646.0        1         0   10/2/2014   \n","4  5324        1981        Married  58293.0        1         0  19-01-2014   \n","\n","   Recency  MntWines  MntFruits  ...  AcceptedCmp1  AcceptedCmp2  Complain  \\\n","0       58       635         88  ...             0             0         0   \n","1       38        11          1  ...             0             0         0   \n","2       26       426         49  ...             0             0         0   \n","3       26        11          4  ...             0             0         0   \n","4       94       173         43  ...             0             0         0   \n","\n","   Response  Marital_Status_LabelEncoded  Education_2n Cycle  Education_Basic  \\\n","0         1                            4               False            False   \n","1         0                            4               False            False   \n","2         0                            5               False            False   \n","3         0                            5               False            False   \n","4         0                            3               False            False   \n","\n","   Education_Graduation  Education_Master  Education_PhD  \n","0                  True             False          False  \n","1                  True             False          False  \n","2                  True             False          False  \n","3                  True             False          False  \n","4                 False             False           True  \n","\n","[5 rows x 32 columns]\n"]}],"source":["import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Correcting the file pathC:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv\n","file_path = r''\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Example: Label Encoding for a single column 'Marital_Status'\n","label_encoder = LabelEncoder()\n","df['Marital_Status_LabelEncoded'] = label_encoder.fit_transform(df['Marital_Status'])\n","\n","# Display the first few rows to verify\n","print(df[['Marital_Status', 'Marital_Status_LabelEncoded']].head())\n","\n","# Repeat the above steps for other categorical columns as needed\n","# Example: One-Hot Encoding for a single column 'Education'\n","df_encoded = pd.get_dummies(df, columns=['Education'])\n","\n","# Display the first few rows to verify\n","print(df_encoded.head())\n","\n","# Repeat the above steps for other categorical columns as needed\n"]},{"cell_type":"markdown","id":"9242e36d","metadata":{"id":"9242e36d"},"source":["### 14. Standardize the columns, so that values are in a particular range"]},{"cell_type":"code","execution_count":18,"id":"72475b68","metadata":{"id":"72475b68"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n","       'Teenhome', 'Dt_Customer', 'Recency', 'MntWines', 'MntFruits',\n","       'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n","       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n","       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n","       'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1',\n","       'AcceptedCmp2', 'Complain', 'Response'],\n","      dtype='object')\n","Columns not found in the dataset.\n"]}],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","\n","# Correcting the file pathC:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv\n","file_path = r''\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Display the columns to verify their names and existence\n","print(df.columns)\n","\n","# Assuming 'Age' and 'Income' are actual columns in your dataset\n","columns_to_standardize = ['Age', 'Income']\n","\n","# Check if columns_to_standardize are in df.columns\n","if all(col in df.columns for col in columns_to_standardize):\n","    # Initialize StandardScaler\n","    scaler = StandardScaler()\n","\n","    # Fit and transform the selected columns\n","    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n","\n","    # Display the first few rows to verify\n","    print(df.head())\n","else:\n","    print(\"Columns not found in the dataset.\")\n","\n"]},{"cell_type":"markdown","id":"d063d2e2","metadata":{"id":"d063d2e2"},"source":["### 15. Apply PCA on the above dataset and determine the number of PCA components to be used so that 90-95% of the variance in data is explained by the same."]},{"cell_type":"code","execution_count":20,"id":"6df3c70e","metadata":{"id":"6df3c70e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n","       'Teenhome', 'Dt_Customer', 'Recency', 'MntWines', 'MntFruits',\n","       'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n","       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n","       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n","       'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1',\n","       'AcceptedCmp2', 'Complain', 'Response'],\n","      dtype='object')\n","Columns not found in the dataset.\n"]}],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","import numpy as np\n","\n","# Correcting the file path\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Display the columns to verify their names and existence\n","print(df.columns)\n","\n","# Assume 'Age' and 'Income' are actual columns in your dataset\n","columns_to_standardize = ['Age', 'Income']\n","\n","# Check if columns_to_standardize are in df.columns\n","if all(col in df.columns for col in columns_to_standardize):\n","    # Initialize StandardScaler\n","    scaler = StandardScaler()\n","\n","    # Standardize the selected columns\n","    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n","\n","    # Apply PCA\n","    pca = PCA()\n","    pca.fit(df)\n","\n","    # Calculate cumulative explained variance ratio\n","    explained_variance_ratio = pca.explained_variance_ratio_\n","    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n","\n","    # Determine the number of components for 90-95% variance explained\n","    n_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1  # +1 because index starts from 0\n","\n","    # Print explained variance ratios and number of components\n","    print(\"Explained Variance Ratio:\")\n","    print(explained_variance_ratio)\n","    print(\"\\nCumulative Explained Variance Ratio:\")\n","    print(cumulative_variance_ratio)\n","    print(f\"\\nNumber of components to explain at least 95% variance: {n_components}\")\n","else:\n","    print(\"Columns not found in the dataset.\")\n"]},{"cell_type":"markdown","id":"b2df19d7","metadata":{"id":"b2df19d7"},"source":["### 16. Apply K-means clustering and segment the data (Use PCA transformed data for clustering)"]},{"cell_type":"code","execution_count":22,"id":"a3a8bb4c","metadata":{"id":"a3a8bb4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n","       'Teenhome', 'Dt_Customer', 'Recency', 'MntWines', 'MntFruits',\n","       'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n","       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n","       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n","       'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1',\n","       'AcceptedCmp2', 'Complain', 'Response'],\n","      dtype='object')\n","Columns not found in the dataset.\n"]}],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","\n","# Correcting the file path\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Display the columns to verify their names and existence\n","print(df.columns)\n","\n","# Assume 'Age' and 'Income' are actual columns in your dataset\n","columns_to_standardize = ['Age', 'Income']\n","\n","# Check if columns_to_standardize are in df.columns\n","if all(col in df.columns for col in columns_to_standardize):\n","    # Initialize StandardScaler\n","    scaler = StandardScaler()\n","\n","    # Standardize the selected columns\n","    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n","\n","    # Apply PCA\n","    pca = PCA(n_components=2)  # Example: choose number of components based on explained variance ratio or requirement\n","    principal_components = pca.fit_transform(df)\n","\n","    # Apply K-means clustering\n","    kmeans = KMeans(n_clusters=3, random_state=42)  # Example: choose number of clusters\n","    kmeans.fit(principal_components)\n","\n","    # Add cluster labels to DataFrame\n","    df['Cluster'] = kmeans.labels_\n","\n","    # Visualize clusters (Example plot for first two principal components)\n","    plt.figure(figsize=(10, 6))\n","    scatter = plt.scatter(principal_components[:, 0], principal_components[:, 1], c=kmeans.labels_, cmap='viridis')\n","    plt.title('K-means Clustering with PCA Components')\n","    plt.xlabel('Principal Component 1')\n","    plt.ylabel('Principal Component 2')\n","    plt.colorbar(scatter, label='Cluster')\n","    plt.show()\n","\n","else:\n","    print(\"Columns not found in the dataset.\")\n"]},{"cell_type":"markdown","id":"d8463aed","metadata":{"id":"d8463aed"},"source":["### 17. Apply Agglomerative clustering and segment the data (Use Original data for clustering), and perform cluster analysis by doing bivariate analysis between the cluster label and different features and write your observations."]},{"cell_type":"code","execution_count":24,"id":"b5ca165b","metadata":{"id":"b5ca165b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n","       'Teenhome', 'Dt_Customer', 'Recency', 'MntWines', 'MntFruits',\n","       'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n","       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n","       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n","       'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1',\n","       'AcceptedCmp2', 'Complain', 'Response'],\n","      dtype='object')\n","Columns not found in the dataset. Check column names or dataset loading.\n"]}],"source":["import pandas as pd\n","\n","# Correcting the file path\n","file_path = r'C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv'\n","\n","# Load the dataset\n","df = pd.read_csv(file_path)\n","\n","# Display the columns to verify their names and existence\n","print(df.columns)\n","\n","# Check if the columns you intend to use ('Age', 'Income', etc.) are in df.columns\n","columns_to_check = ['Age', 'Income']\n","\n","if all(col in df.columns for col in columns_to_check):\n","    # Proceed with your data preprocessing and clustering steps here\n","    print(\"Columns found in the dataset. Proceed with further steps.\")\n","else:\n","    # Print an error message or handle the case where columns are not found\n","    print(\"Columns not found in the dataset. Check column names or dataset loading.\")\n"]},{"cell_type":"markdown","id":"797a5ecd","metadata":{"id":"797a5ecd"},"source":["### Visualization and Interpretation of results"]},{"cell_type":"code","execution_count":30,"id":"d1e75760","metadata":{"id":"d1e75760"},"outputs":[{"ename":"ValueError","evalue":"Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[30], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assume data preprocessing and K-means clustering have been performed\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Apply K-means clustering\u001b[39;00m\n\u001b[0;32m     12\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIncome\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Visualize clusters\u001b[39;00m\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1070\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \n\u001b[0;32m   1050\u001b[0m \u001b[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;124;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1464\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m \n\u001b[0;32m   1440\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1464\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1475\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n","\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"]}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.cluster import KMeans\n","\n","# Load the dataset\n","df = pd.read_csv('C:/Users/hp/Downloads/Lab 4 - Unsu[ervised learinng/marketing.csv')\n","\n","# Assume data preprocessing and K-means clustering have been performed\n","\n","# Apply K-means clustering\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","df['Cluster'] = kmeans.fit_predict(df[['ID', 'Income']])\n","\n","# Visualize clusters\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot( x='ID',y='Income', hue='Cluster', data=df, palette='viridis', legend='full')\n","plt.title('K-means Clustering: Age vs Income by Cluster')\n","plt.xlabel('ID')\n","plt.ylabel('Income')\n","plt.show()\n","\n","# Optional: Print cluster centers or perform additional analysis\n","print(\"\\nCluster Centers:\")\n","print(kmeans.cluster_centers_)\n","\n","# Explore other visualizations and interpretations as per your analysis needs\n"]},{"cell_type":"markdown","id":"36afd95b","metadata":{"id":"36afd95b"},"source":["-----\n","## Happy Learning\n","-----"]}],"metadata":{"colab":{"collapsed_sections":["36afd95b"],"name":"Unsupervised Learning - Lab session.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}
